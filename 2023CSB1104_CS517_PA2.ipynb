{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f7b6250-7121-492b-aa4c-39b83bd68451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "from scipy.spatial import Delaunay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c5756e-f7da-4ed6-a4fa-d3d2ad78199a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get 468 tie points for human faces using mediapipe\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_landmarks_mp\u001b[39m(image):\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Initialize MediaPipe Face Mesh.\u001b[39;00m\n\u001b[32m      7\u001b[39m     mp_face_mesh = mp.solutions.face_mesh\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "# Get 468 tie points for human faces using mediapipe\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "def find_landmarks_mp(image):\n",
    "    # Initialize MediaPipe Face Mesh.\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True,\n",
    "                                      max_num_faces=1,\n",
    "                                      refine_landmarks=True,\n",
    "                                      min_detection_confidence=0.5)\n",
    "    \n",
    "    # Convert the BGR image to RGB as MediaPipe expects RGB input.\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the image to detect face landmarks.\n",
    "    results = face_mesh.process(rgb_image)\n",
    "    \n",
    "    arr = []\n",
    "    \n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            print(type(face_landmarks))  # Similar to the print statement in the original code.\n",
    "            height, width, _ = image.shape\n",
    "            \n",
    "            # Loop through each detected landmark and convert normalized coordinates to pixel coordinates.\n",
    "            for landmark in face_landmarks.landmark:\n",
    "                x = int(landmark.x * width)\n",
    "                y = int(landmark.y * height)\n",
    "                arr.append([x, y])\n",
    "    \n",
    "    # Add image corners for smoother boundaries.\n",
    "    arr.append([0, 0])\n",
    "    arr.append([0, image.shape[0]])\n",
    "    arr.append([image.shape[1], 0])\n",
    "    arr.append([image.shape[1], image.shape[0]])\n",
    "    \n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff39fa-f6a8-4e4b-a4fe-f8c9f1440f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Get 68 landmark points using dlib\n",
    "\n",
    "\n",
    "import dlib\n",
    "\n",
    "def find_landmarks_dlib(image):\n",
    "    # Path to the shape predictor file and input image\n",
    "    PREDICTOR_PATH = \"shape_predictor_68_face_landmarks.dat\"\n",
    "    \n",
    "    # Initialize dlib's face detector (HOG-based) and create the landmark predictor\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(PREDICTOR_PATH)\n",
    "    \n",
    "    # Load the image and convert it to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the grayscale image\n",
    "    faces = detector(gray)\n",
    "    arr = []\n",
    "    # Loop over each face detected\n",
    "    for face in faces:\n",
    "        # Determine the facial landmarks for the face region\n",
    "        landmarks = predictor(gray, face)\n",
    "        print(type(landmarks))\n",
    "        \n",
    "        # Loop through each of the 68 facial landmarks and draw them on the image\n",
    "        for i in range(0, 68):\n",
    "            x = landmarks.part(i).x\n",
    "            y = landmarks.part(i).y\n",
    "            arr.append([x, y])\n",
    "    # Add the corners for smoother. REMINDER TO TRYR ADDING MIDPOINTS!!!!!!!!!\n",
    "    arr.append([0,0])\n",
    "    arr.append([0,image.shape[0]])\n",
    "    arr.append([image.shape[1], 0])\n",
    "    arr.append([image.shape[1], image.shape[0]])\n",
    "\n",
    "    return arr\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754fb15-4528-4f7f-9fca-17087631f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point(image, window_title, existing_points):\n",
    "    \"\"\"\n",
    "    1. Shows the image with the previously marked points for reference\n",
    "    2. Waits for click input of a new point or q to exiit\n",
    "    \"\"\"\n",
    "    # Create a copy of the image and draw the existing points.\n",
    "    disp_img = image.copy()\n",
    "    for pt in existing_points:\n",
    "        cv2.circle(disp_img, pt, 3, (0, 255, 0), -1)\n",
    "    \n",
    "    new_point = []\n",
    "\n",
    "    def on_mouse(event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            new_point.append((x, y))\n",
    "\n",
    "    cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)\n",
    "    cv2.setMouseCallback(window_title, on_mouse)\n",
    "    cv2.imshow(window_title, disp_img)\n",
    "    \n",
    "    # Wait until a point is clicked or 'q' is pressed.\n",
    "    while True:\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        # If a point has been clicked, break out.\n",
    "        if new_point:\n",
    "            break\n",
    "        # If 'q' is pressed, break out.\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.destroyWindow(window_title)\n",
    "    return new_point[0] if new_point else None\n",
    "\n",
    "def mpa(src_image_path, dst_image_path):\n",
    "    \"\"\"\n",
    "    1. We get list of landmark points from mediapipe\n",
    "    2. We extend that list with click input\n",
    "    \"\"\"\n",
    "    src_img = cv2.imread(src_image_path)\n",
    "    dst_img = cv2.imread(dst_image_path)\n",
    "    \n",
    "    if src_img is None or dst_img is None:\n",
    "        raise ValueError(\"Could not load one or both images.\")\n",
    "\n",
    "    src_points = find_landmarks_mp(src_img)\n",
    "    dst_points = find_landmarks_mp(dst_img)\n",
    "    if len(src_points) != len(dst_points):\n",
    "        src_points = []\n",
    "        dst_points = []\n",
    "    \n",
    "        # Append the corners by default\n",
    "        src_points.append([0,0])\n",
    "        src_points.append([0,src_img.shape[0]])\n",
    "        src_points.append([src_img.shape[1], 0])\n",
    "        src_points.append([src_img.shape[1], src_img.shape[0]])\n",
    "        dst_points.append([0,0])\n",
    "        dst_points.append([0,dst_img.shape[0]])\n",
    "        dst_points.append([dst_img.shape[1], 0])\n",
    "        dst_points.append([dst_img.shape[1], dst_img.shape[0]])\n",
    "        \n",
    "    \n",
    "    print(\"For each pair, click on the SOURCE image then the corresponding point on the DESTINATION image.\")\n",
    "    \n",
    "    print(\"Press 'q' in any window to finish input.\")\n",
    "    \n",
    "    while True:\n",
    "        # Get a point from the source image.\n",
    "        pt_src = get_point(src_img, \"Source Image\", src_points)\n",
    "        if pt_src is None:\n",
    "            print(\"Finished input for source image.\")\n",
    "            break\n",
    "        src_points.append(pt_src)\n",
    "        print(f\"Selected source point: {pt_src}\")\n",
    "        \n",
    "        # Get the corresponding point from the destination image.\n",
    "        pt_dst = get_point(dst_img, \"Destination Image\", dst_points)\n",
    "        if pt_dst is None:\n",
    "            print(\"Finished input for destination image.\")\n",
    "            break\n",
    "        dst_points.append(pt_dst)\n",
    "        print(f\"Selected destination point: {pt_dst}\")\n",
    "\n",
    " \n",
    "    \n",
    "    return src_points, dst_points\n",
    "\n",
    "def dliba(src_image_path, dst_image_path):\n",
    "    \"\"\"\n",
    "    1. We get list of landmark points from dlib\n",
    "    2. We extend that list with click input\n",
    "    \"\"\"\n",
    "    src_img = cv2.imread(src_image_path)\n",
    "    dst_img = cv2.imread(dst_image_path)\n",
    "    \n",
    "    if src_img is None or dst_img is None:\n",
    "        raise ValueError(\"Could not load one or both images.\")\n",
    "\n",
    "    src_points = find_landmarks_dlib(src_img)\n",
    "    dst_points = find_landmarks_dlib(dst_img)\n",
    "    if len(src_points) != len(dst_points):\n",
    "        src_points = []\n",
    "        dst_points = []\n",
    "    \n",
    "        # Append the corners by default\n",
    "        src_points.append([0,0])\n",
    "        src_points.append([0,src_img.shape[0]])\n",
    "        src_points.append([src_img.shape[1], 0])\n",
    "        src_points.append([src_img.shape[1], src_img.shape[0]])\n",
    "        dst_points.append([0,0])\n",
    "        dst_points.append([0,dst_img.shape[0]])\n",
    "        dst_points.append([dst_img.shape[1], 0])\n",
    "        dst_points.append([dst_img.shape[1], dst_img.shape[0]])\n",
    "        \n",
    "    \n",
    "    print(\"For each pair, click on the SOURCE image then the corresponding point on the DESTINATION image.\")\n",
    "    print(\"Press 'q' in any window to finish input.\")\n",
    "    \n",
    "    while True:\n",
    "        # Get a point from the source image.\n",
    "        pt_src = get_point(src_img, \"Source Image\", src_points)\n",
    "        if pt_src is None:\n",
    "            print(\"Finished input for source image.\")\n",
    "            break\n",
    "        src_points.append(pt_src)\n",
    "        print(f\"Selected source point: {pt_src}\")\n",
    "        \n",
    "        # Get the corresponding point from the destination image.\n",
    "        pt_dst = get_point(dst_img, \"Destination Image\", dst_points)\n",
    "        if pt_dst is None:\n",
    "            print(\"Finished input for destination image.\")\n",
    "            break\n",
    "        dst_points.append(pt_dst)\n",
    "        print(f\"Selected destination point: {pt_dst}\")\n",
    "\n",
    " \n",
    "    \n",
    "    return src_points, dst_points\n",
    "\n",
    "def clickity(src_image_path, dst_image_path):\n",
    "    \"\"\"\n",
    "    Takes the click inputs and returns the list\n",
    "    \"\"\"\n",
    "    src_img = cv2.imread(src_image_path)\n",
    "    dst_img = cv2.imread(dst_image_path)\n",
    "    \n",
    "    if src_img is None or dst_img is None:\n",
    "        raise ValueError(\"Could not load one or both images.\")\n",
    "\n",
    "    src_points = []\n",
    "    dst_points = []\n",
    "\n",
    "    # Append the corners by default\n",
    "    src_points.append([0,0])\n",
    "    src_points.append([0,src_img.shape[0]])\n",
    "    src_points.append([src_img.shape[1], 0])\n",
    "    src_points.append([src_img.shape[1], src_img.shape[0]])\n",
    "    dst_points.append([0,0])\n",
    "    dst_points.append([0,dst_img.shape[0]])\n",
    "    dst_points.append([dst_img.shape[1], 0])\n",
    "    dst_points.append([dst_img.shape[1], dst_img.shape[0]])\n",
    "        \n",
    "    \n",
    "    print(\"For each pair, click on the SOURCE image then the corresponding point on the DESTINATION image.\")\n",
    "    print(\"Press 'q' in any window to finish input.\")\n",
    "    \n",
    "    while True:\n",
    "        # Get a point from the source image.\n",
    "        pt_src = get_point(src_img, \"Source Image\", src_points)\n",
    "        if pt_src is None:\n",
    "            print(\"Finished input for source image.\")\n",
    "            break\n",
    "        src_points.append(pt_src)\n",
    "        print(f\"Selected source point: {pt_src}\")\n",
    "        \n",
    "        # Get the corresponding point from the destination image.\n",
    "        pt_dst = get_point(dst_img, \"Destination Image\", dst_points)\n",
    "        if pt_dst is None:\n",
    "            print(\"Finished input for destination image.\")\n",
    "            break\n",
    "        dst_points.append(pt_dst)\n",
    "        print(f\"Selected destination point: {pt_dst}\")\n",
    "\n",
    " \n",
    "    \n",
    "    return src_points, dst_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a5b962-4a9b-4db8-ab3b-e6d327522bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def warp_triangle(src_img, dst_img, out_img, tri_src, tri_dst, tri_interp, blend):\n",
    "    \"\"\"\n",
    "    Warp one triangle from the source and destination images into an intermediate shape\n",
    "    and blend their colors. \n",
    "    \"\"\"\n",
    "    # Convert triangles to NumPy arrays\n",
    "    tri_interp = np.array(tri_interp, dtype=np.float32)\n",
    "    tri_src    = np.array(tri_src, dtype=np.float32)\n",
    "    tri_dst    = np.array(tri_dst, dtype=np.float32)\n",
    "    \n",
    "    # Get bounding box for the intermediate triangle\n",
    "    x, y, w, h = cv2.boundingRect(tri_interp)\n",
    "    \n",
    "    # Create a grid of (x, y) coordinates for the bounding box; add 0.5 for pixel centers\n",
    "    grid_x, grid_y = np.meshgrid(np.arange(x, x+w), np.arange(y, y+h))\n",
    "    grid_points = np.vstack((grid_x.ravel(), grid_y.ravel())).T + 0.5  # shape (N, 2)\n",
    "    \n",
    "    # Unpack vertices of the intermediate triangle\n",
    "    A, B, C = tri_interp\n",
    "    \n",
    "    # Compute denominator for barycentric coordinates (same for all points)\n",
    "    denom = ( (B[1]-C[1])*(A[0]-C[0]) + (C[0]-B[0])*(A[1]-C[1]) )\n",
    "    if np.abs(denom) < 1e-6:\n",
    "        return  # Avoid division by zero for degenerate triangles.\n",
    "    \n",
    "    # Vectorized computation of barycentrics for all grid points\n",
    "    w1 = ((B[1]-C[1])*(grid_points[:,0]-C[0]) + (C[0]-B[0])*(grid_points[:,1]-C[1])) / denom\n",
    "    w2 = ((C[1]-A[1])*(grid_points[:,0]-C[0]) + (A[0]-C[0])*(grid_points[:,1]-C[1])) / denom\n",
    "    w3 = 1 - w1 - w2\n",
    "    \n",
    "    # Mask: select only points that are inside the intermediate triangle\n",
    "    mask = (w1 >= 0) & (w2 >= 0) & (w3 >= 0)\n",
    "    if np.sum(mask) == 0:\n",
    "        return  # No valid points.\n",
    "    \n",
    "    valid_points = grid_points[mask]      # (M, 2)\n",
    "    valid_w1 = w1[mask][:, np.newaxis]      # (M, 1)\n",
    "    valid_w2 = w2[mask][:, np.newaxis]\n",
    "    valid_w3 = w3[mask][:, np.newaxis]\n",
    "    \n",
    "    # Map points to corresponding positions in source and destination triangles\n",
    "    src_coords = valid_w1 * tri_src[0] + valid_w2 * tri_src[1] + valid_w3 * tri_src[2]\n",
    "    dst_coords = valid_w1 * tri_dst[0] + valid_w2 * tri_dst[1] + valid_w3 * tri_dst[2]\n",
    "    \n",
    "    # Vectorized bilinear interpolation\n",
    "    def bilinear_interp_vec(img, coords):\n",
    "        h_img, w_img, channels = img.shape\n",
    "        x = coords[:, 0]\n",
    "        y = coords[:, 1]\n",
    "        x0 = np.floor(x).astype(np.int32)\n",
    "        x1 = x0 + 1\n",
    "        y0 = np.floor(y).astype(np.int32)\n",
    "        y1 = y0 + 1\n",
    "\n",
    "        # Clip coordinates to image boundaries.\n",
    "        x0 = np.clip(x0, 0, w_img - 1)\n",
    "        x1 = np.clip(x1, 0, w_img - 1)\n",
    "        y0 = np.clip(y0, 0, h_img - 1)\n",
    "        y1 = np.clip(y1, 0, h_img - 1)\n",
    "\n",
    "        Ia = img[y0, x0]  # Top-left\n",
    "        Ib = img[y0, x1]  # Top-right\n",
    "        Ic = img[y1, x0]  # Bottom-left\n",
    "        Id = img[y1, x1]  # Bottom-right\n",
    "\n",
    "        # Compute interpolation weights.\n",
    "        wa = ((x1 - x) * (y1 - y))[:, np.newaxis]\n",
    "        wb = ((x - x0) * (y1 - y))[:, np.newaxis]\n",
    "        wc = ((x1 - x) * (y - y0))[:, np.newaxis]\n",
    "        wd = ((x - x0) * (y - y0))[:, np.newaxis]\n",
    "\n",
    "        return wa * Ia + wb * Ib + wc * Ic + wd * Id\n",
    "\n",
    "    src_colors = bilinear_interp_vec(src_img, src_coords)\n",
    "    dst_colors = bilinear_interp_vec(dst_img, dst_coords)\n",
    "    \n",
    "    # Blend colors from source and destination\n",
    "    blended_colors = (1 - blend) * src_colors + blend * dst_colors\n",
    "    \n",
    "    # Write blended colors back into output image\n",
    "    # Convert floating coordinates to integer indices (subtract 0.5 to reverse our earlier offset)\n",
    "    valid_points_int = np.round(valid_points - 0.5).astype(np.int32)\n",
    "    xs = valid_points_int[:, 0]\n",
    "    ys = valid_points_int[:, 1]\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! TEST THIS !!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    h_img, w_img, _ = out_img.shape\n",
    "    xs = np.clip(xs, 0, w_img - 1)\n",
    "    ys = np.clip(ys, 0, h_img - 1)\n",
    "\n",
    "    out_img[ys, xs] = blended_colors\n",
    "\n",
    "def morph_images(src_img, dst_img, pts_src, pts_dst, num_frames=30, gif_name='morph.gif'):\n",
    "    # Convert images to float32 for processing.\n",
    "    src_img = src_img.astype(np.float32)\n",
    "    dst_img = dst_img.astype(np.float32)\n",
    "    pts_src = np.array(pts_src, np.float32)\n",
    "    pts_dst = np.array(pts_dst, np.float32)\n",
    "    \n",
    "    # Compute Delaunay triangulation using source points.\n",
    "    delaunay = Delaunay(pts_src)\n",
    "    triangles = delaunay.simplices  # Use these triangle indices for both images.\n",
    "    \n",
    "    frames = []\n",
    "    h, w = src_img.shape[:2]\n",
    "    \n",
    "    # Loop over each frame.\n",
    "    for f in range(num_frames + 1):\n",
    "        blend = f / num_frames  # 0 means source, 1 means destination.\n",
    "        morphed_frame = np.zeros_like(src_img)\n",
    "        \n",
    "        # Process each triangle.\n",
    "        for tri_indices in triangles:\n",
    "            tri_src = [pts_src[i] for i in tri_indices]\n",
    "            tri_dst = [pts_dst[i] for i in tri_indices]\n",
    "            # Compute the intermediate triangle by linearly blending vertex positions.\n",
    "            tri_interp = [\n",
    "                (1 - blend) * np.array(tri_src[i]) + blend * np.array(tri_dst[i])\n",
    "                for i in range(3)\n",
    "            ]\n",
    "            warp_triangle(src_img, dst_img, morphed_frame, tri_src, tri_dst, tri_interp, blend)\n",
    "        \n",
    "        # Clip pixel values and convert to uint8.\n",
    "        frame_uint8 = np.clip(morphed_frame, 0, 255).astype(np.uint8)\n",
    "        frames.append(frame_uint8)\n",
    "        print(f\"Frame {f}/{num_frames} complete.\")\n",
    "    \n",
    "    # Save all frames as an animated GIF.\n",
    "    imageio.mimsave(gif_name, frames, duration=0.025)\n",
    "    print(f\"Saved GIF as {gif_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa38fe4-e62e-4c23-aaf8-84e7a29923e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ties_from_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    # The first line should contain T\n",
    "    T = int(lines[0])\n",
    "\n",
    "    tie_points_A = []\n",
    "    tie_points_B = []\n",
    "\n",
    "    # Next T lines each contain xA, yA, xB, yB\n",
    "    for i in range(1, T + 1):\n",
    "        coords = lines[i].split()\n",
    "        if len(coords) != 4:\n",
    "            raise ValueError(f\"Line {i} doesn't have exactly 4 values: {lines[i]}\")\n",
    "        \n",
    "        xA = float(coords[0])\n",
    "        yA = float(coords[1])\n",
    "        xB = float(coords[2])\n",
    "        yB = float(coords[3])\n",
    "        \n",
    "        tie_points_A.append((xA, yA))\n",
    "        tie_points_B.append((xB, yB))\n",
    "\n",
    "    return tie_points_A, tie_points_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc428923-f4b0-4e92-84c4-f0ed658359a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def read_points_from_file(file_path, delimiter=' '):\n",
    "    points = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                # Skip empty lines\n",
    "                continue\n",
    "\n",
    "            # If a delimiter is specified, use it. Otherwise, try to auto-detect space or comma.\n",
    "            if delimiter is not None:\n",
    "                coords = line.split(delimiter)\n",
    "            else:\n",
    "                # Attempt to split on space first, then fallback to comma\n",
    "                if ' ' in line:\n",
    "                    coords = line.split()\n",
    "                else:\n",
    "                    coords = line.split(',')\n",
    "\n",
    "            if len(coords) == 2:\n",
    "                x, y = float(coords[0]), float(coords[1])\n",
    "                points.append((x, y))\n",
    "            else:\n",
    "                raise ValueError(f\"Line format is invalid: '{line}'\")\n",
    "\n",
    "    return points\n",
    "\n",
    "def read_tie_points(tie1_path='tie1.txt', tie2_path='tie2.txt'):\n",
    "    \"\"\"\n",
    "    Reads tie1.txt and tie2.txt each containing an array of points and returns two lists of (x, y) tuples.\n",
    "    \"\"\"\n",
    "    tie1_points = read_points_from_file(tie1_path)\n",
    "    tie2_points = read_points_from_file(tie2_path)\n",
    "    return tie1_points, tie2_points\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1db82-2cf5-4865-b178-d58e0776a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(src_image_path, dst_image_path, tie_points_acquisition=\"mp\"):\n",
    "    \"\"\"\n",
    "    tie_points_acquisition can be:\n",
    "    \n",
    "        1. mp : Use mediapipe to find the landmark poitns\n",
    "        2. dlib: Use dlib to find the landmark points.\n",
    "        3. mpa: Mediapipe assisted (add more points with clicks)\n",
    "        4. dliba: Dlib assisted (add more points with clicks)\n",
    "        5. click: Add all points with clicks\n",
    "        6. txt: Read .txt file for tie pionts of each image\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the images\n",
    "    img1 = cv2.imread(src_image_path)\n",
    "    img2 = cv2.imread(dst_image_path)\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    " \n",
    "    # points1 = find_landmarks(img1)\n",
    "    # points2 = find_landmarks(img2)\n",
    "    if tie_points_acquisition == \"mp\":\n",
    "            print(\"Using MediaPipe for landmark points.\")\n",
    "            points1 = find_landmarks_mp(img1)\n",
    "            points2 = find_landmarks_mp(img2)\n",
    "           \n",
    "\n",
    "    elif tie_points_acquisition == \"dlib\":\n",
    "            print(\"Using Dlib for landmark points.\")\n",
    "            points1 = find_landmarks_dlib(img1)\n",
    "            points2 = find_landmarks_dlib(img2)\n",
    "\n",
    "    elif tie_points_acquisition == \"mpa\":\n",
    "            print(\"Using Mediapipe + manual clicks for additional points.\")\n",
    "            points1, points2 = mpa(src_image_path, dst_image_path)\n",
    "            \n",
    "    elif tie_points_acquisition == \"dliba\":\n",
    "            print(\"Using Dlib + manual clicks for additional points.\")\n",
    "            points1, points2 = dliba(src_image_path, dst_image_path)\n",
    "\n",
    "    elif tie_points_acquisition == \"click\":\n",
    "            print(\"Manually adding all points with clicks.\")\n",
    "            points1, points2 = clickity(src_image_path, dst_image_path)\n",
    "\n",
    "    elif tie_points_acquisition == \"txt\":\n",
    "            print(\"Reading tie points from tie.txt\")\n",
    "            points1, points2 = read_ties_from_file('tie.txt')\n",
    "            \n",
    "    else:\n",
    "            raise ValueError(\"Unsupported tie_points_acquisition option\")\n",
    "    \n",
    "    \n",
    "    # Generate the morphing GIF.\n",
    "    morph_images(img1, img2, points1, points2, num_frames=60, gif_name='morph.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa52dd-c9ed-48f8-8406-ee11af0376d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    src_image_path = \"image1.png\"\n",
    "    dst_image_path = \"image2.png\"\n",
    "    \n",
    "    tie_point_acquisition = input(\"Choose method of tie point acquisition: \")\n",
    "    \n",
    "    main(src_image_path, dst_image_path, tie_point_acquisition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
